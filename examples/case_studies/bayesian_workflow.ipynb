{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016911,
     "end_time": "2020-03-27T06:09:14.400757",
     "exception": false,
     "start_time": "2020-03-27T06:09:14.383846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(bayesian_workflow)=\n",
    "\n",
    "# The Bayesian Workflow\n",
    "\n",
    ":::{post} Jun 16, 2025\n",
    ":tags: workflow\n",
    ":category: intermediate, how-to\n",
    ":author: Thomas Wiecki, Chris Fonnesbeck\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 2.069288,
     "end_time": "2020-03-27T06:09:16.527404",
     "exception": false,
     "start_time": "2020-03-27T06:09:14.458116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import arviz as az\n",
    "import load_covid_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "# plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "sampler_kwargs = {\"chains\": 4, \"cores\": 4, \"tune\": 2000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strengths of Bayesian statistics that are critical here:\n",
    "* Great flexibility to quickly and iteratively build statistical models\n",
    "* Offers principled way of dealing with uncertainty\n",
    "* Don't just want most likely outcome but distribution of all possible outcomes\n",
    "* Allows expert information to guide model by using informative priors\n",
    "\n",
    "In this section you'll learn:\n",
    "* How to go from data to a model idea\n",
    "* How to find priors for your model\n",
    "* How to evaluate a model\n",
    "* How to iteratively improve a model\n",
    "* How to forecast into the future\n",
    "* How powerful generative modeling can be"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009784,
     "end_time": "2020-03-27T06:09:16.547140",
     "exception": false,
     "start_time": "2020-03-27T06:09:16.537356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load data\n",
    "\n",
    "First we'll load data on COVID-19 cases from the WHO. In order to ease analysis we will remove any days were confirmed cases was below 100 (as reporting is often very noisy in this time frame). It also allows us to align countries with each other for easier comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.663552,
     "end_time": "2020-03-27T06:09:18.220032",
     "exception": false,
     "start_time": "2020-03-27T06:09:16.556480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_covid_data.load_data(drop_states=True, filter_n_days_100=2)\n",
    "countries = df.country.unique()\n",
    "n_countries = len(countries)\n",
    "df = df.loc[lambda x: (x.days_since_100 >= 0)]\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Workflow\n",
    "\n",
    "Next, we will start developing a model of the spread. These models will start out simple (and poor) but we will iteratively improve them. A good workflow to adopt when developing your own models is:\n",
    "\n",
    "1. Plot the data\n",
    "2. Build model\n",
    "3. Run prior predictive check\n",
    "4. Fit model\n",
    "5. Assess convergence\n",
    "6. Run posterior predictive check\n",
    "7. Improve model\n",
    "\n",
    "### 1. Plot the data\n",
    "\n",
    "We will look at German COVID-19 cases. At first, we will only look at the first 30 days after Germany crossed 100 cases, later we will look at the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = \"Germany\"\n",
    "date = \"2020-07-31\"\n",
    "df_country = df.query(f'country==\"{country}\"').loc[:date].iloc[:30]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "df_country.confirmed.plot(ax=ax)\n",
    "ax.set(ylabel=\"Confirmed cases\", title=country)\n",
    "sns.despine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the above plot and think of what type of model you would build to model the data.\n",
    "\n",
    "### 2. Build model\n",
    "\n",
    "The above line kind of looks exponential. This matches with knowledge from epidemiology whereas early in an epidemic it grows exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time-range of days since 100 cases were crossed\n",
    "t = df_country.days_since_100.values\n",
    "# Get number of confirmed cases for Germany\n",
    "confirmed = df_country.confirmed.values\n",
    "\n",
    "with pm.Model() as model_exp1:\n",
    "    # Intercept\n",
    "    a = pm.Normal(\"a\", mu=0, sigma=100)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.Normal(\"b\", mu=0.3, sigma=0.3)\n",
    "\n",
    "    # Exponential regression\n",
    "    growth = a * (1 + b) ** t\n",
    "\n",
    "    # Error term\n",
    "    eps = pm.HalfNormal(\"eps\", 100)\n",
    "\n",
    "    # Likelihood\n",
    "    pm.Normal(\"obs\", mu=growth, sigma=eps, observed=confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model_exp1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at the above model, what do you think? Is there anything you would have done differently?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run prior predictive check\n",
    "\n",
    "Without even fitting the model to our data, we generate new potential data from our priors. Usually we have less intuition about the parameter space, where we define our priors, and more intution about what data we might expect to see. A prior predictive check thus allows us to make sure the model can generate the types of data we expect to see.\n",
    "\n",
    "The process works as follows:\n",
    "\n",
    "1. Pick a point from the prior $\\theta_i$\n",
    "2. Generate data set $x_i \\sim f(\\theta_i)$ where $f$ is our likelihood function (e.g. normal).\n",
    "3. Rinse and repeat $n$ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp1:\n",
    "    prior_pred = pm.sample_prior_predictive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(prior_pred.prior_predictive[\"obs\"].values.squeeze().T, color=\"0.5\", alpha=0.1)\n",
    "ax.set(\n",
    "    ylim=(-1000, 1000),\n",
    "    xlim=(0, 10),\n",
    "    title=\"Prior predictive\",\n",
    "    xlabel=\"Days since 100 cases\",\n",
    "    ylabel=\"Positive cases\",\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this look sensible? Why or why not? What is the prior predictive sample telling us?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's wrong with this model?\n",
    "\n",
    "Above you hopefully identified a few issues with this model:\n",
    "1. Cases can't be negative\n",
    "2. Cases can not start at 0, as we set it to start at above 100.\n",
    "3. Case counts can't go down\n",
    "\n",
    "Let's improve our model. The presence of negative cases is due to us using a Normal likelihood. Instead, let's use a `NegativeBinomial`, which is similar to `Poisson` which is commonly used for count-data but has an extra dispersion parameter that allows more flexiblity in modeling the variance of the data.\n",
    "\n",
    "We will also change the prior of the intercept to be centered at 100 and tighten the prior of the slope.\n",
    "\n",
    "The negative binomial distribution uses an overdispersion parameter, which we will describe using a gamma distribution. A companion package called `preliz`, a library for prior distribution elicitation, has a nice utility called `maxent` that will help us parameterize this prior, as the gamma distribution is not as intuitive to work with as the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preliz as pz\n",
    "\n",
    "gamma_params = pz.maxent(pz.Gamma(), lower=0.1, upper=20, mass=0.95)\n",
    "gamma_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pm.draw(pm.Gamma.dist(alpha=2, beta=0.2), 1000), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_country.days_since_100.values\n",
    "confirmed = df_country.confirmed.values\n",
    "\n",
    "with pm.Model() as model_exp2:\n",
    "    # Intercept\n",
    "    a = pm.Normal(\"a\", mu=100, sigma=25)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.Normal(\"b\", mu=0.3, sigma=0.1)\n",
    "\n",
    "    # Exponential regression\n",
    "    growth = a * (1 + b) ** t\n",
    "\n",
    "    alpha = pz.maxent(pz.Gamma(), lower=0.1, upper=20, mass=0.95, plot=False).to_pymc(\"alpha\")\n",
    "\n",
    "    # Likelihood\n",
    "    pm.NegativeBinomial(\"obs\", growth, alpha=alpha, observed=confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp2:\n",
    "    prior_pred = pm.sample_prior_predictive()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(prior_pred.prior_predictive[\"obs\"].values.squeeze().T, color=\"0.5\", alpha=0.1)\n",
    "ax.set(\n",
    "    ylim=(-100, 1000),\n",
    "    xlim=(0, 10),\n",
    "    title=\"Prior predictive\",\n",
    "    xlabel=\"Days since 100 cases\",\n",
    "    ylabel=\"Positive cases\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp2:\n",
    "    trace_exp2 = pm.sample(**sampler_kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much better. However, we can include even more prior information. For example, we know that the intercept *can not* be below 100 because of how we sliced the data. We can thus create a prior that does not have probability mass below 100. For this, we use the PyMC `HalfNormal` distribution; we can apply the same for the slope which we know is not going to be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_country.days_since_100.values\n",
    "confirmed = df_country.confirmed.values\n",
    "\n",
    "with pm.Model() as model_exp3:\n",
    "    # Intercept\n",
    "    a0 = pm.HalfNormal(\"a0\", sigma=25)\n",
    "    a = pm.Deterministic(\"a\", a0 + 100)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.HalfNormal(\"b\", sigma=0.2)\n",
    "\n",
    "    # Exponential regression\n",
    "    growth = a * (1 + b) ** t\n",
    "\n",
    "    gamma_params = pm.find_constrained_prior(\n",
    "        pm.Gamma, lower=0.1, upper=20, init_guess={\"alpha\": 6, \"beta\": 1}, mass=0.95\n",
    "    )\n",
    "    alpha = pm.Gamma(\"alpha\", **gamma_params)\n",
    "\n",
    "    # Likelihood\n",
    "    pm.NegativeBinomial(\"obs\", growth, alpha=alpha, observed=confirmed)\n",
    "\n",
    "    prior_pred = pm.sample_prior_predictive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(prior_pred.prior[\"a\"].squeeze(), legend=False)\n",
    "plt.title(\"Prior of a\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(prior_pred.prior[\"b\"].squeeze(), legend=False)\n",
    "plt.title(\"Prior of b\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "ax.plot(az.extract(prior_pred.prior_predictive)[\"obs\"], color=\"0.5\", alpha=0.1)\n",
    "ax.set(\n",
    "    ylim=(0, 1000),\n",
    "    xlim=(0, 10),\n",
    "    title=\"Prior predictive\",\n",
    "    xlabel=\"Days since 100 cases\",\n",
    "    ylabel=\"Positive cases\",\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even though the intercept parameter can not be below 100 now, we still see data generated at below hundred. Why? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp3:\n",
    "    # Inference button (TM)\n",
    "    trace_exp3 = pm.sample(**sampler_kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Assess convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_exp3, var_names=[\"a\", \"b\", \"alpha\"])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_exp3, var_names=[\"a\", \"b\", \"alpha\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(trace_exp3);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison\n",
    "\n",
    "Let's quickly compare the two models we were able to sample from.\n",
    "\n",
    "Model comparison requires the log-likelihoods of the respective models. For efficiency, these are not computed automatically, so we need to manually calculate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp2:\n",
    "    pm.compute_log_likelihood(trace_exp2)\n",
    "\n",
    "with model_exp3:\n",
    "    pm.compute_log_likelihood(trace_exp3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the ArviZ `compare` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = az.compare({\"exp2\": trace_exp2, \"exp3\": trace_exp3})\n",
    "az.plot_compare(comparison)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like bounding the priors did not result in better fit. This is not unexpected because our change in prior was very small. We will still continue with `model_exp3` because we have prior information that these parameters are bounded in this way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Run posterior predictive check\n",
    "\n",
    "Similar to the prior predictive, we can also generate new data by repeatedly taking samples from the posterior and generating data using these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp3:\n",
    "    # Draw sampels from posterior predictive\n",
    "    post_pred = pm.sample_posterior_predictive(trace_exp3.posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(\n",
    "    post_pred.posterior_predictive[\"obs\"].sel(chain=0).values.squeeze().T, color=\"0.5\", alpha=0.05\n",
    ")\n",
    "ax.plot(confirmed, color=\"r\", label=\"data\")\n",
    "ax.set(\n",
    "    xlabel=\"Days since 100 cases\",\n",
    "    ylabel=\"Confirmed cases (log scale)\",\n",
    "    # ylim=(0, 100_000),\n",
    "    title=country,\n",
    "    yscale=\"log\",\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that does not look terrible, the data is at least inside of what the model can produce. Let's look at residuals for systematic errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "resid = post_pred.posterior_predictive[\"obs\"].sel(chain=0) - confirmed\n",
    "ax.plot(resid.T, color=\"0.5\", alpha=0.01)\n",
    "ax.set(ylim=(-50_000, 200_000), ylabel=\"Residual\", xlabel=\"Days since 100 cases\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you see?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and forecasting\n",
    "\n",
    "We might also be interested in predicting on unseen or data, or, in the case time-series data like here, in forecasting. In `PyMC` you can do so easily using `pm.Data` nodes. What it allows you to do is define data to a PyMC model that you can later switch out for other data. That way, when you for example do posterior predictive sampling, it will generate samples into the future.\n",
    "\n",
    "Let's change our model to use `pm.Data` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_exp4:\n",
    "    # pm.Data needs to be in the model context so that we can\n",
    "    # keep track of it.\n",
    "    # Then, we can then use it like any other array.\n",
    "    t_data = pm.Data(\"t\", df_country.days_since_100.values)\n",
    "    confirmed_data = pm.Data(\"confirmed\", df_country.confirmed.values)\n",
    "\n",
    "    # Intercept\n",
    "    a0 = pm.HalfNormal(\"a0\", sigma=25)\n",
    "    a = pm.Deterministic(\"a\", a0 + 100)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.HalfNormal(\"b\", sigma=0.2)\n",
    "\n",
    "    # Exponential regression\n",
    "    growth = a * (1 + b) ** t_data\n",
    "\n",
    "    # Likelihood\n",
    "    pm.NegativeBinomial(\n",
    "        \"obs\", growth, alpha=pm.Gamma(\"alpha\", mu=6, sigma=1), observed=confirmed_data\n",
    "    )\n",
    "\n",
    "    trace_exp4 = pm.sample(**sampler_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp4:\n",
    "    # Update our data containers.\n",
    "    # Recall that because confirmed is observed, we do not\n",
    "    # need to specify any data, as that is only needed\n",
    "    # during inference. But do have to update it to match\n",
    "    # the shape.\n",
    "    pm.set_data({\"t\": np.arange(60), \"confirmed\": np.zeros(60, dtype=\"int\")})\n",
    "\n",
    "    post_pred = pm.sample_posterior_predictive(trace_exp4.posterior)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we held data back before, we can now see how the predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(\n",
    "    post_pred.posterior_predictive[\"obs\"].sel(chain=0).squeeze().values.T, color=\"0.5\", alpha=0.05\n",
    ")\n",
    "ax.plot(df_country.confirmed.values, color=\"r\", label=\"in-sample\")\n",
    "df_confirmed = df.query(f'country==\"{country}\"').loc[:date, \"confirmed\"]\n",
    "ax.plot(\n",
    "    np.arange(29, len(df_confirmed)),\n",
    "    df_confirmed.iloc[29:].values,\n",
    "    color=\"b\",\n",
    "    label=\"out-of-sample\",\n",
    ")\n",
    "ax.set(xlabel=\"Days since 100 cases\", ylabel=\"Confirmed cases\", title=country, yscale=\"log\")\n",
    "ax.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Improve model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic model\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/courses-images-archive-read-only/wp-content/uploads/sites/924/2015/11/25202016/CNX_Precalc_Figure_04_07_0062.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = df.query(f'country==\"{country}\"').loc[:date]\n",
    "\n",
    "with pm.Model() as logistic_model:\n",
    "    t_data = pm.Data(\"t\", df_country.days_since_100.values)\n",
    "    confirmed_data = pm.Data(\"confirmed\", df_country.confirmed.values)\n",
    "\n",
    "    # Intercept\n",
    "    a0 = pm.HalfNormal(\"a0\", sigma=25)\n",
    "    intercept = pm.Deterministic(\"intercept\", a0 + 100)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.HalfNormal(\"b\", sigma=0.2)\n",
    "\n",
    "    carrying_capacity = pm.Uniform(\"carrying_capacity\", lower=1_000, upper=80_000_000)\n",
    "    # Transform carrying_capacity to a\n",
    "    a = carrying_capacity / intercept - 1\n",
    "\n",
    "    # Logistic\n",
    "    growth = carrying_capacity / (1 + a * pm.math.exp(-b * t_data))\n",
    "\n",
    "    # Likelihood\n",
    "    pm.NegativeBinomial(\n",
    "        \"obs\", growth, alpha=pm.Gamma(\"alpha\", mu=6, sigma=1), observed=confirmed_data\n",
    "    )\n",
    "\n",
    "    prior_pred = pm.sample_prior_predictive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(prior_pred.prior_predictive[\"obs\"].squeeze().values.T, color=\"0.5\", alpha=0.1)\n",
    "ax.set(\n",
    "    title=\"Prior predictive\",\n",
    "    xlabel=\"Days since 100 cases\",\n",
    "    ylabel=\"Positive cases\",\n",
    "    yscale=\"log\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logistic_model:\n",
    "    # Inference\n",
    "    trace_logistic = pm.sample(**sampler_kwargs, target_accept=0.9)\n",
    "\n",
    "    # Sample posterior predcitive\n",
    "    pm.sample_posterior_predictive(trace_logistic, extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_logistic)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(\n",
    "    trace_logistic.posterior_predictive[\"obs\"].sel(chain=0).squeeze().values.T,\n",
    "    color=\"0.5\",\n",
    "    alpha=0.05,\n",
    ")\n",
    "ax.plot(df_confirmed.values, color=\"r\")\n",
    "ax.set(xlabel=\"Days since 100 cases\", ylabel=\"Confirmed cases\", title=country);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "resid = (\n",
    "    trace_logistic.posterior_predictive[\"obs\"].sel(chain=0).squeeze().values - df_confirmed.values\n",
    ")\n",
    "ax.plot(resid.T, color=\"0.5\", alpha=0.01)\n",
    "ax.set(ylabel=\"Residual\", xlabel=\"Days since 100 cases\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between the residuals from before?\n",
    "\n",
    "#### Model comparison\n",
    "\n",
    "In order to compare our models we first need to refit with the longer data we now have. Fortunately we can easily swap out the data because these are `pm.Data` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp4:\n",
    "    pm.set_data({\"t\": df_country.days_since_100.values, \"confirmed\": df_country.confirmed.values})\n",
    "\n",
    "    trace_exp4_full = pm.sample(**sampler_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp4:\n",
    "    pm.compute_log_likelihood(trace_exp4_full)\n",
    "\n",
    "with logistic_model:\n",
    "    pm.compute_log_likelihood(trace_logistic)\n",
    "\n",
    "az.plot_compare(az.compare({\"exp4\": trace_exp4_full, \"logistic\": trace_logistic}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the logistic model provides a much better fit to the data. \n",
    "\n",
    "Although there is still some small bias in the residuals but overall we might think our model is quite good. Let's see how it does on a different country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = \"US\"\n",
    "# df_country = df.loc[lambda x: (x.country == country)]\n",
    "df_country = df.query(f'country==\"{country}\"').loc[:date]\n",
    "df_confirmed = df_country[\"confirmed\"]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "df_country.confirmed.plot(ax=ax)\n",
    "ax.set(ylabel=\"Confirmed cases\", title=country)\n",
    "sns.despine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data looks quite different. Let's see how our logistic model fits this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_confirmed = df.loc[lambda x: (x.country == country), 'confirmed']\n",
    "df_confirmed = df.query(f'country==\"{country}\"').loc[:date, \"confirmed\"]\n",
    "\n",
    "with pm.Model() as logistic_model:\n",
    "    t_data = pm.Data(\"t\", df_country.days_since_100.values)\n",
    "    confirmed_data = pm.Data(\"confirmed\", df_country.confirmed.values)\n",
    "\n",
    "    # Intercept\n",
    "    a0 = pm.HalfNormal(\"a0\", sigma=25)\n",
    "    intercept = pm.Deterministic(\"intercept\", a0 + 100)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.HalfNormal(\"b\", sigma=0.2)\n",
    "\n",
    "    carrying_capacity = pm.Uniform(\"carrying_capacity\", lower=1_000, upper=100_000_000)\n",
    "    # Transform carrying_capacity to a\n",
    "    a = carrying_capacity / intercept - 1\n",
    "\n",
    "    # Logistic\n",
    "    growth = carrying_capacity / (1 + a * pm.math.exp(-b * t_data))\n",
    "\n",
    "    # Likelihood\n",
    "    pm.NegativeBinomial(\n",
    "        \"obs\", growth, alpha=pm.Gamma(\"alpha\", mu=6, sigma=1), observed=confirmed_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logistic_model:\n",
    "    trace_logistic_us = pm.sample(**sampler_kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we see some problems with sampling which should make us suspicious that this model might not be the best for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_logistic_us)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logistic_model:\n",
    "    pm.sample_posterior_predictive(trace_logistic_us, extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(\n",
    "    trace_logistic_us.posterior_predictive[\"obs\"].sel(chain=0).squeeze().values.T,\n",
    "    color=\"0.5\",\n",
    "    alpha=0.05,\n",
    ")\n",
    "ax.plot(df_confirmed.values, color=\"r\")\n",
    "ax.set(xlabel=\"Days since 100 cases\", ylabel=\"Confirmed cases\", title=country);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model is not a great fit to this data. Why? What assumptions does the model make about the spread of COVID-19?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "- Originally authored by Thomas Wiecki in 2020\n",
    "- Adapted and expanded by Chris Fonnesbeck in June 2025 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ":::{include} ../page_footer.md\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "papermill": {
   "duration": 1049.211513,
   "end_time": "2020-03-27T06:26:42.767376",
   "environment_variables": {},
   "exception": null,
   "input_path": "2020-03-16-covid19_growth_bayes.ipynb",
   "output_path": "2020-03-16-covid19_growth_bayes.ipynb",
   "parameters": {},
   "start_time": "2020-03-27T06:09:13.555863",
   "version": "2.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
